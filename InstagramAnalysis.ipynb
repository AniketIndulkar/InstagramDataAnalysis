{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# Available styles\n",
    "# ['Solarize_Light2', '_classic_test_patch', '_mpl-gallery', '_mpl-gallery-nogrid', 'bmh', 'classic', 'dark_background', 'fast', 'fivethirtyeight', 'ggplot', 'grayscale', 'seaborn-v0_8', 'seaborn-v0_8-bright', 'seaborn-v0_8-colorblind', 'seaborn-v0_8-dark', 'seaborn-v0_8-dark-palette', 'seaborn-v0_8-darkgrid', 'seaborn-v0_8-deep', 'seaborn-v0_8-muted', 'seaborn-v0_8-notebook', 'seaborn-v0_8-paper', 'seaborn-v0_8-pastel', 'seaborn-v0_8-poster', 'seaborn-v0_8-talk', 'seaborn-v0_8-ticks', 'seaborn-v0_8-white', 'seaborn-v0_8-whitegrid', 'tableau-colorblind10']\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import PassiveAggressiveRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import GradientBoostingRegressors\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"/Users/aniketindulkar/Documents/GitHub/MLProjects/InstagramAnalysis/Instagram.csv\",\n",
    "                    encoding = 'latin1')\n",
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the style globally for all plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "\n",
    "# Initialize a figure with 3 subplots (vertically arranged)\n",
    "fig, axs = plt.subplots(2, 2, figsize=(15, 10))  # 2 rows, 2 column\n",
    "\n",
    "# Plot \"Distribution of Impressions From Home\"\n",
    "sns.histplot(data['From Home'], ax=axs[0,0], kde=False)\n",
    "axs[0,0].set_title(\"Distribution of Impressions From Home\")\n",
    "\n",
    "# Plot \"Distribution of Impressions From Hashtags\"\n",
    "sns.histplot(data['From Hashtags'], ax=axs[0,1], kde=False)\n",
    "axs[0,1].set_title(\"Distribution of Impressions From Hashtags\")\n",
    "\n",
    "# Plot \"Distribution of Impressions From Explore\"\n",
    "sns.histplot(data['From Explore'], ax=axs[1,0], kde=False)\n",
    "axs[1,0].set_title(\"Distribution of Impressions From Explore\")\n",
    "\n",
    "# Plot \"Distribution of Impressions From Explore\"\n",
    "sns.histplot(data['From Other'], ax=axs[1,1], kde=False)\n",
    "axs[1,1].set_title(\"Distribution of Impressions From Explore\")\n",
    "\n",
    "# Adjust the layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the combined plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum the impressions coming from the 'From Home' column of the DataFrame\n",
    "home = data[\"From Home\"].sum()\n",
    "\n",
    "# Sum the impressions coming from the 'From Hashtags' column of the DataFrame\n",
    "hashtags = data[\"From Hashtags\"].sum()\n",
    "\n",
    "# Sum the impressions coming from the 'From Explore' column of the DataFrame\n",
    "explore = data[\"From Explore\"].sum()\n",
    "\n",
    "# Sum the impressions coming from the 'Other' sources column of the DataFrame\n",
    "other = data[\"From Other\"].sum()\n",
    "\n",
    "# Define the labels for the pie chart segments\n",
    "labels = ['From Home', 'From Hashtags', 'From Explore', 'Other']\n",
    "\n",
    "# Aggregate the summed values into a list that corresponds to the labels\n",
    "values = [home, hashtags, explore, other]\n",
    "\n",
    "# Create a pie chart using Plotly Express\n",
    "# - `data`: (Although not directly used for plotting, passed contextually)\n",
    "# - `values`: data points for each segment of the pie chart\n",
    "# - `names`: labels for each pie chart segment\n",
    "# - `title`: title of the chart\n",
    "# - `hole`: creates a donut-like pie chart with the hole's radius set to 0.5\n",
    "fig = px.pie(data, values=values, names=labels, \n",
    "             title='Impressions on Instagram Posts From Various Sources', hole=0.5)\n",
    "\n",
    "# Display the figure\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join all entries in the 'Caption' column into a single string separated by spaces\n",
    "text = \" \".join(i for i in data.Caption)\n",
    "\n",
    "# Create a set of stopwords that the WordCloud generator will ignore\n",
    "# Stopwords are common words that do not carry significant meaning, such as \"the\", \"and\", etc.\n",
    "stopwords = set(STOPWORDS)\n",
    "\n",
    "# Initialize the WordCloud object with stopwords and background color set to white\n",
    "# `generate(text)` constructs the word cloud from the provided text string\n",
    "wordcloud = WordCloud(stopwords=stopwords, background_color=\"white\").generate(text)\n",
    "\n",
    "# Set the style of plotting to 'classic' for a more traditional look with no gridlines or background color\n",
    "plt.style.use('classic')\n",
    "\n",
    "# Create a figure object with the specified size in inches (width, height)\n",
    "plt.figure(figsize=(12,10))\n",
    "\n",
    "# Display the word cloud image with 'bilinear' interpolation which smooths the displayed image\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "\n",
    "# Remove the x and y axis labels to create a cleaner look\n",
    "plt.axis(\"off\")\n",
    "\n",
    "# Display the plot with the word cloud\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \" \".join(i for i in data.Hashtags)\n",
    "wordcloud = WordCloud(stopwords=stopwords, background_color=\"white\").generate(text)\n",
    "plt.figure( figsize=(12,10))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = px.scatter(data_frame = data, x=\"Impressions\",\n",
    "                    y=\"Likes\", size=\"Likes\", trendline=\"ols\", \n",
    "                    title = \"Relationship Between Likes and Impressions\")\n",
    "figure.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = px.scatter(data_frame = data, x=\"Impressions\",\n",
    "                    y=\"Comments\", size=\"Comments\", trendline=\"ols\", \n",
    "                    title = \"Relationship Between Comments and Total Impressions\")\n",
    "figure.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = px.scatter(data_frame = data, x=\"Impressions\",\n",
    "                    y=\"Shares\", size=\"Shares\", trendline=\"ols\", \n",
    "                    title = \"Relationship Between Shares and Total Impressions\")\n",
    "figure.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = px.scatter(data_frame = data, x=\"Impressions\",\n",
    "                    y=\"Saves\", size=\"Saves\", trendline=\"ols\", \n",
    "                    title = \"Relationship Between Post Saves and Total Impressions\")\n",
    "figure.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_numeric = data.apply(pd.to_numeric, errors='coerce')\n",
    "correlation = data_numeric.corr()\n",
    "print(correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversion_rate = (data[\"Follows\"].sum() / data[\"Profile Visits\"].sum()) * 100\n",
    "print(conversion_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = px.scatter(data_frame = data, x=\"Profile Visits\",\n",
    "                    y=\"Follows\", size=\"Follows\", trendline=\"ols\", \n",
    "                    title = \"Relationship Between Profile Visits and Followers Gained\")\n",
    "figure.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert selected columns into a NumPy array to be used as features for a machine learning model\n",
    "# This includes 'Likes', 'Saves', 'Comments', 'Shares', 'Profile Visits', and 'Follows' from the DataFrame\n",
    "x = np.array(data[['Likes', 'Saves', 'Comments', 'Shares', 'Profile Visits', 'Follows']])\n",
    "\n",
    "# Convert the 'Impressions' column into a NumPy array to be used as the target variable for the model\n",
    "y = np.array(data[\"Impressions\"])\n",
    "\n",
    "# Split the dataset into training and testing sets using sklearn's train_test_split function\n",
    "# The dataset is split into 80% training data and 20% testing data\n",
    "# 'test_size=0.2' specifies that 20% of the data should be used for the test set\n",
    "# 'random_state=42' is set to ensure reproducibility of results, as it controls the shuffling applied to the data before the split\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Passive Aggressive Regressor** is typically used for large-scale learning and is particularly well-suited for scenarios where you have continuously arriving data (streaming data). It is called \"passive-aggressive\" because the algorithm remains passive for correct predictions (i.e., when the prediction is correct or the error is within a margin), but turns aggressive when a prediction is incorrect, making adjustments to correct the mistake.\n",
    "\n",
    "### Key Characteristics:\n",
    "- **Online Learning**: This model updates continuously as new data arrives, making it suitable for systems that ingest data in a streaming fashion.\n",
    "- **Adaptability**: The model can quickly adapt to new patterns in the data, which makes it robust in environments where data patterns can change over time.\n",
    "- **Margin Infraction**: The model updates itself not only when it predicts incorrectly but also when the prediction is not confident enough (i.e., when the prediction does not exceed a margin threshold).\n",
    "- **Usage**: It's commonly used in tasks like real-time decision-making and situations where the model needs to quickly adapt to new data without retraining from scratch.\n",
    "\n",
    "The **PassiveAggressiveRegressor** is used here to predict a continuous target variable based on several input features. The fitting process adjusts the model weights based on the training data, and the performance is evaluated using the R^2 statistic, which measures the proportion of variance in the dependent variable that is predictable from the independent variables. This score can range from -∞ to 1, where a value closer to 1 indicates a better fit to the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Passive Aggressive Regressor model\n",
    "model = PassiveAggressiveRegressor()\n",
    "\n",
    "# Fit the model on the training dataset\n",
    "# This step involves the model learning the relationship between the input features (xtrain) and the target variable (ytrain)\n",
    "model.fit(xtrain, ytrain)\n",
    "\n",
    "# Evaluate the model on the testing dataset\n",
    "# The score method returns the coefficient of determination R^2 of the prediction,\n",
    "# which is a measure of how well the unseen samples are predicted by the model\n",
    "score = model.score(xtest, ytest)\n",
    "print(\"Model R^2 Score:\", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression Model\n",
    "\n",
    "**Linear Regression** is a foundational statistical method used in machine learning for predictive modeling. It aims to model the relationship between a scalar dependent variable `y` and one or more independent variables (or \"predictors\") denoted `X`.\n",
    "\n",
    "#### How It Works:\n",
    "- The model works by estimating coefficients for the linear equation, involving one or more independent variables that best predict the dependent variable.\n",
    "- The linear equation can be expressed as:\n",
    "  \\[ y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + ... + \\beta_nx_n + \\epsilon \\]\n",
    "  where \\( \\beta_0, \\beta_1, ..., \\beta_n \\) are coefficients, and \\( \\epsilon \\) is the error term.\n",
    "\n",
    "#### Key Characteristics:\n",
    "- **Simplicity**: Linear regression is straightforward to understand and explain, making it a good starting point for predictive modeling.\n",
    "- **Interpretability**: Each coefficient in the model explains the influence of one independent variable on the dependent variable.\n",
    "- **Assumptions**: The effectiveness of the model depends on whether certain assumptions are met, including linearity, independence, homoscedasticity, and normal distribution of errors.\n",
    "\n",
    "#### Usage:\n",
    "- It is widely used in both statistics and machine learning for tasks such as trend forecasting, determining the strength of predictors, and forecasting an effect.\n",
    "- Common applications include economics (predicting economic growth), business (forecasting sales), and health sciences (predicting outcomes of treatments).\n",
    "\n",
    "#### Evaluation:\n",
    "- The model's performance can be evaluated using the R² statistic, which measures the proportion of variance in the dependent variable that is predictable from the independent variables. A higher R² indicates a better fit to the data.\n",
    "\n",
    "This model is particularly useful when there is a linear relationship between the input variables and the target output. Despite its simplicity, linear regression can provide powerful insights into data and is a staple in the toolbox of any data analyst or scientist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Linear Regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Fit the model on the training dataset\n",
    "# This step involves training the model by finding the best coefficients for the input features that minimize the error in predicting the target variable\n",
    "model.fit(xtrain, ytrain)\n",
    "\n",
    "# Evaluate the model on the testing dataset\n",
    "# The score method returns the coefficient of determination R^2 of the prediction,\n",
    "# which measures the proportion of variance in the dependent variable that is predictable from the independent variables\n",
    "model_score = model.score(xtest, ytest)\n",
    "print(\"Model R^2 Score:\", model_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression Model\n",
    "\n",
    "**Ridge Regression**, also known as Tikhonov regularization, is a technique used for analyzing multiple regression data that suffer from multicollinearity. By introducing a degree of bias to the regression estimates, Ridge Regression reduces the standard errors.\n",
    "\n",
    "#### How It Works:\n",
    "- Ridge Regression adds a penalty equivalent to the square of the magnitude of the coefficients to the least squares cost function. This penalty is parameterized by `alpha`.\n",
    "- The mathematical formulation is:\n",
    "  \\[ J(\\beta) = ||Y - X\\beta||^2 + \\alpha||\\beta||^2 \\]\n",
    "  where \\( J(\\beta) \\) is the cost function, \\( \\beta \\) represents the coefficient matrix, \\( X \\) is the feature matrix, \\( Y \\) is the target vector, and \\( \\alpha \\) is the regularization term.\n",
    "\n",
    "#### Key Characteristics:\n",
    "- **Regularization**: The key feature of Ridge Regression is its ability to reduce model complexity by penalizing large coefficients through the `alpha` parameter.\n",
    "- **Shrinkage**: As `alpha` increases, the flexibility of the Ridge model decreases, leading to a decrease in variance but a slight increase in bias.\n",
    "- **Multicollinearity**: It is particularly useful when data features are multicollinear or when the feature dimension is high relative to the number of data points.\n",
    "\n",
    "#### Usage:\n",
    "- Commonly used in scenarios where the number of predictor variables in a set exceeds the number of observations, or when a data set has multicollinearity (i.e., independent variables are highly correlated).\n",
    "- Employed in various fields including economics, biology, and engineering, particularly where predictive accuracy and robustness against multicollinearity are important.\n",
    "\n",
    "#### Evaluation:\n",
    "- The effectiveness of Ridge Regression is typically measured by the R² statistic on the test data, which provides an indication of how well unseen samples are likely to be predicted.\n",
    "- The choice of `alpha` significantly affects the model’s performance, and it is often selected via cross-validation.\n",
    "\n",
    "Ridge Regression offers a robust alternative to standard linear regression, especially useful in situations where a simple least squares estimate proves inadequate or unstable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Ridge Regression model with a regularization strength of alpha=1.0\n",
    "# Alpha is a parameter that controls the amount of shrinkage: the larger the value of alpha, the greater the amount of shrinkage\n",
    "model = Ridge(alpha=1.0)\n",
    "\n",
    "# Fit the Ridge Regression model on the training dataset\n",
    "# This involves adjusting the model parameters to minimize the regularized loss function, which includes a penalty for the size of the coefficients\n",
    "model.fit(xtrain, ytrain)\n",
    "\n",
    "# Evaluate the model on the testing dataset\n",
    "# The score method returns the coefficient of determination R^2 of the prediction,\n",
    "# indicating the proportion of variance in the dependent variable predictable from the independent variables\n",
    "model_score = model.score(xtest, ytest)\n",
    "print(\"Model R^2 Score:\", model_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso Regression Model\n",
    "\n",
    "**Lasso Regression** (Least Absolute Shrinkage and Selection Operator) is a type of linear regression that uses shrinkage. Shrinkage is where data values are shrunk towards a central point, like the mean. The Lasso procedures encourage simple, sparse models (i.e., models with fewer parameters).\n",
    "\n",
    "#### How It Works:\n",
    "- Lasso Regression adds a penalty equal to the absolute value of the magnitude of coefficients to the loss function. This type of regularization (L1) can lead to zero coefficients in some variables, effectively performing variable selection.\n",
    "- The mathematical formulation is:\n",
    "  \\[ J(\\beta) = \\frac{1}{2n} ||Y - X\\beta||^2 + \\alpha||\\beta||_1 \\]\n",
    "  where \\( J(\\beta) \\) is the cost function, \\( \\beta \\) represents the coefficient matrix, \\( X \\) is the feature matrix, \\( Y \\) is the target vector, \\( n \\) is the number of samples, and \\( \\alpha \\) is the regularization strength.\n",
    "\n",
    "#### Key Characteristics:\n",
    "- **Variable Selection**: Lasso Regression can yield sparse models where only a subset of the predictors are used, which is useful for models that benefit from variable reduction.\n",
    "- **Regularization**: By adjusting `alpha`, users can control the impact of the penalty on the model complexity. Higher values of `alpha` force more coefficients to zero.\n",
    "- **Multicollinearity Handling**: Similar to Ridge Regression, Lasso can manage multicollinearity among predictors, although its method of shrinking coefficients can differ significantly.\n",
    "\n",
    "#### Usage:\n",
    "- Lasso is widely used when many features are present, but only some of them are expected to be important for prediction. This is typical in fields like genomics where the number of predictors (genes) can be very large compared to the number of observations.\n",
    "- Employed in various predictive models where model simplicity and interpretability are essential.\n",
    "\n",
    "#### Evaluation:\n",
    "- The performance of Lasso Regression is also measured by the R² statistic on the test data, giving a sense of fit quality.\n",
    "- The selection of `alpha` is critical and can be fine-tuned using techniques like cross-validation.\n",
    "\n",
    "Lasso Regression is particularly useful for creating parsimonious models in situations where a simple and interpretable solution is preferable to a complex one. Its ability to reduce the number of predictor variables by setting some coefficient estimates to zero helps in identifying the most significant features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Lasso Regression model with a regularization strength of alpha=0.1\n",
    "# Alpha is a parameter that controls the amount of shrinkage: the larger the value of alpha, \n",
    "# the greater the amount of shrinkage and thus the more coefficients are driven to zero\n",
    "model = Lasso(alpha=0.1)\n",
    "\n",
    "# Fit the Lasso Regression model on the training dataset\n",
    "# This involves adjusting the model parameters to minimize the loss function,\n",
    "# which includes a penalty proportional to the absolute value of the coefficients\n",
    "model.fit(xtrain, ytrain)\n",
    "\n",
    "# Evaluate the model on the testing dataset\n",
    "# The score method returns the coefficient of determination R^2 of the prediction,\n",
    "# which indicates how well the model performs compared to a simple mean of the target values\n",
    "model_score = model.score(xtest, ytest)\n",
    "print(\"Model R^2 Score:\", model_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Regression Model\n",
    "\n",
    "**Decision Tree Regression** utilizes a decision tree (a flowchart-like structure) to model the decision-making process over continuous data. It is a non-linear model that is powerful for capturing complex patterns in data sets, particularly useful when linear assumptions do not hold.\n",
    "\n",
    "#### How It Works:\n",
    "- Decision Tree models split the input data into subsets based on decisions made upon feature values. These decisions are represented as nodes in the tree.\n",
    "- The tree is constructed by recursively partitioning data into branches, which represents an inference about the target values to be predicted.\n",
    "- The mathematical goal is to reduce variability in predictions by splitting the dataset on the feature that results in the highest decrease in sum of squared error (SSE).\n",
    "\n",
    "#### Key Characteristics:\n",
    "- **Interpretability**: One of the most significant advantages of decision trees is their ease of interpretation. They can be visualized graphically and understood even by non-experts.\n",
    "- **Non-parametric**: They do not assume any distribution of the data, making them suitable for non-linear data patterns.\n",
    "- **Control Over-fitting**: `max_depth` controls the size of the tree to prevent overfitting. Smaller trees are less complex and generalize better.\n",
    "\n",
    "#### Usage:\n",
    "- Decision trees are widely used in real-world applications where relationships between features and outcomes are complex and non-linear.\n",
    "- They are popular in finance for pricing options, in energy for predicting load, and in operations for predicting failure times of machines.\n",
    "\n",
    "#### Evaluation:\n",
    "- The model’s performance is evaluated using the R² statistic, which measures the proportion of variance in the dependent variable that is predictable from the independent variables.\n",
    "- Performance can be further enhanced by pruning the tree after it has been built, which involves removing parts of the tree that provide little power in predicting target variables.\n",
    "\n",
    "Decision Tree Regression is useful in scenarios where data relationships are intricate and an understandable model is crucial for decision making. Its ability to break down data hierarchically fits well with human decision-making processes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Decision Tree Regressor model with a maximum depth of 5\n",
    "# The `max_depth` parameter controls the maximum number of levels in the tree, which helps prevent overfitting\n",
    "model = DecisionTreeRegressor(max_depth=5)\n",
    "\n",
    "# Fit the Decision Tree Regressor model on the training dataset\n",
    "# This involves building a tree that models the relationship between the input features and the target variable\n",
    "model.fit(xtrain, ytrain)\n",
    "\n",
    "# Evaluate the model on the testing dataset\n",
    "# The score method returns the coefficient of determination R^2 of the prediction,\n",
    "# indicating how well the predictions approximate the true data values\n",
    "model_score = model.score(xtest, ytest)\n",
    "print(\"Model R^2 Score:\", model_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Regression (SVR) Model\n",
    "\n",
    "**Support Vector Regression (SVR)** applies the principles of support vector machines (SVM) to regression problems. It attempts to fit the best line within a threshold error margin and is well-suited for both linear and non-linear data, depending on the kernel used.\n",
    "\n",
    "#### How It Works:\n",
    "- SVR performs linear regression in a high-dimensional feature space using kernel tricks. The objective is to find a function that deviates from the targets by a value no greater than a specified margin ε, while being as flat as possible.\n",
    "- The RBF (Radial Basis Function) kernel used here allows the SVR model to handle non-linear relationships by mapping input features into higher-dimensional spaces where a linear separator might exist.\n",
    "\n",
    "#### Key Characteristics:\n",
    "- **Flexibility in Handling Non-linearity**: The choice of kernel (e.g., linear, polynomial, RBF) determines the ability of the SVR model to handle non-linear relationships.\n",
    "- **Margin of Error Tunability**: The ε-insensitive tube (margin of error) within which no penalty is given to errors offers a unique advantage in controlling what differences are considered errors.\n",
    "- **Regularization**: The regularization parameter (C) can be tuned to optimize the trade-off between achieving a low error on the training data and minimizing the model complexity for better generalization.\n",
    "\n",
    "#### Usage:\n",
    "- SVR is commonly used in financial markets for predicting prices, in energy consumption for load forecasting, and in any sector requiring robustness against outliers or the capability to model non-linear phenomena effectively.\n",
    "\n",
    "#### Evaluation:\n",
    "- Performance is typically measured using the coefficient of determination (R²), which reflects the proportion of variance in the target variable explained by the independent variables in the model.\n",
    "- Due to its complexity and computational cost, proper parameter tuning (kernel type, C, ε) is critical for the performance and efficiency of the model.\n",
    "\n",
    "SVR is a powerful tool when it comes to regression tasks involving complex datasets where traditional regression methods might fall short. Its ability to model non-linear relationships and control over the error margin make it a versatile option for many predictive modeling scenarios.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Support Vector Regression (SVR) model with the RBF (Radial Basis Function) kernel\n",
    "# The 'rbf' kernel is a popular choice for SVR and allows the model to handle non-linear data\n",
    "model = SVR(kernel='rbf')\n",
    "\n",
    "# Fit the SVR model on the training dataset\n",
    "# This involves finding the hyperplane (or set of hyperplanes in high-dimensional space) that best fits the data\n",
    "model.fit(xtrain, ytrain)\n",
    "\n",
    "# Evaluate the model on the testing dataset\n",
    "# The score method returns the coefficient of determination R^2 of the prediction,\n",
    "# indicating how well the predictions match the actual values of the dependent variable\n",
    "model_score = model.score(xtest, ytest)\n",
    "print(\"Model R^2 Score:\", model_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForest Regression Model\n",
    "\n",
    "**RandomForest Regression** is an ensemble learning method that operates by constructing a multitude of decision trees at training time and outputting the average prediction of the individual trees. It combines the simplicity of decision trees with flexibility, resulting in higher accuracy without a significant increase in computational complexity.\n",
    "\n",
    "#### How It Works:\n",
    "- **Ensemble Method**: RandomForest builds multiple decision trees and merges them together to get a more accurate and stable prediction.\n",
    "- **Training Process**: Each tree in the forest is built from a random sample of the training set, which helps in making the model robust against overfitting.\n",
    "- **Prediction**: When making predictions, the RandomForest averages the predictions of the individual trees to improve accuracy and control over-fitting.\n",
    "\n",
    "#### Key Characteristics:\n",
    "- **Robustness**: Due to its method of averaging multiple trees, RandomForest is less prone to overfitting than a single decision tree.\n",
    "- **Handle Large Data Sets with Higher Dimensionality**: It can handle thousands of input variables without variable deletion, which is great for cases where features are on the higher side.\n",
    "- **Versatility**: Useful for both classification and regression tasks and does well on large datasets.\n",
    "\n",
    "#### Usage:\n",
    "- RandomForest is widely used across many industries including banking (for credit scoring and fraud detection), healthcare (for identifying diseases and predicting drug responses), and e-commerce (for predicting customer behavior and preferences).\n",
    "\n",
    "#### Evaluation:\n",
    "- **R² Score**: Reflects the proportion of variance in the dependent variable that is predictable from the independent variables.\n",
    "- **Parameter Tuning**: Performance can often be increased by tuning parameters such as the number of trees in the forest, the number of features considered for splitting at each leaf node, etc.\n",
    "\n",
    "RandomForest Regression is especially popular due to its ease of use, performance, and robustness, making it a preferred choice for many predictive modeling tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the RandomForestRegressor model with 100 trees in the forest\n",
    "# n_estimators=100 specifies that the forest should consist of 100 trees\n",
    "model = RandomForestRegressor(n_estimators=100)\n",
    "\n",
    "# Fit the RandomForest model on the training dataset\n",
    "# This involves building multiple decision trees during the training phase and outputting the mean prediction of the individual trees\n",
    "model.fit(xtrain, ytrain)\n",
    "\n",
    "# Evaluate the model on the testing dataset\n",
    "# The score method returns the coefficient of determination R^2 of the prediction,\n",
    "# which indicates how well the model explains the variation in the dependent variable from the independent variables\n",
    "model_score = model.score(xtest, ytest)\n",
    "print(\"Model R^2 Score:\", model_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting Regression Model\n",
    "\n",
    "**Gradient Boosting Regression** is an advanced ensemble technique that builds models sequentially, each correcting its predecessor, thus improving the predictive accuracy of the final model.\n",
    "\n",
    "#### How It Works:\n",
    "- **Sequential Modeling**: Gradient Boosting builds the model in a stage-wise fashion. It constructs new models that predict the residuals or errors of prior models and then adds these new models to the ensemble.\n",
    "- **Loss Minimization**: Each new model is fitted on the remaining unexplained variance (errors) of the previous models in a greedy algorithm fashion, minimizing a loss function.\n",
    "\n",
    "#### Key Characteristics:\n",
    "- **Flexibility**: Can be used with different loss functions, making it adaptable to regression and classification problems.\n",
    "- **Handling Overfitting**: The learning rate and the number of estimators are crucial parameters. A smaller learning rate and more estimators can lead to better performance, but at the risk of overfitting if not tuned properly.\n",
    "- **Feature Importance**: Gradient Boosting can identify important features, making it valuable for feature selection.\n",
    "\n",
    "#### Usage:\n",
    "- Widely used in industries like finance for risk assessment and predictive modeling, in marketing for customer lifetime value predictions, and in medical fields for predictive diagnosis.\n",
    "- Often used in machine learning competitions due to its effectiveness in handling varied types of data and its ability to produce highly accurate models.\n",
    "\n",
    "#### Evaluation:\n",
    "- **R² Score**: Used to measure the proportion of variance in the dependent variable that is predictable from the independent variables.\n",
    "- **Model Tuning**: Parameters like the number of trees, depth of trees, learning rate, and type of loss function greatly influence the performance and need careful tuning.\n",
    "\n",
    "Gradient Boosting Regression is favored for its performance and predictive accuracy across a wide range of data types and is considered one of the most powerful techniques available for predictive modeling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Gradient Boosting Regressor model with specific settings\n",
    "# n_estimators=100 specifies that the model should use 100 boosting stages to produce a robust prediction model\n",
    "# learning_rate=0.1 controls the contribution of each tree to the final outcome and can be used to fine-tune the performance of the model\n",
    "model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1)\n",
    "\n",
    "# Fit the Gradient Boosting Regressor model on the training dataset\n",
    "# This involves sequentially adding models to correct the residuals of prior models\n",
    "model.fit(xtrain, ytrain)\n",
    "\n",
    "# Evaluate the model on the testing dataset\n",
    "# The score method returns the coefficient of determination R^2 of the prediction,\n",
    "# indicating how well the model performs on unseen data\n",
    "model_score = model.score(xtest, ytest)\n",
    "print(\"Model R^2 Score:\", model_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features = [['Likes','Saves', 'Comments', 'Shares', 'Profile Visits', 'Follows']]\n",
    "features = np.array([[282.0, 233.0, 4.0, 9.0, 165.0, 54.0]])\n",
    "model.predict(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features and targets from the dataset\n",
    "X = data[['Impressions', 'From Home', 'From Hashtags', 'From Explore',\n",
    "          'From Other', 'Saves', 'Profile Visits', 'Follows', 'Caption', 'Hashtags']]\n",
    "y = data[['Likes', 'Shares', 'Comments']]\n",
    "\n",
    "# Handling categorical data: Encoding 'Caption' and 'Hashtags'\n",
    "# Define which features are categorical\n",
    "categorical_features = ['Caption', 'Hashtags']\n",
    "# Setup the transformer for encoding: OneHotEncoder will convert categorical variables into a form that could be provided\n",
    "# to ML algorithms to do a better job in prediction\n",
    "categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "# Setup the preprocessor with ColumnTransformer to apply transformations to different columns\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', categorical_transformer, categorical_features)  # Apply OneHotEncoder to categorical features\n",
    "    ], remainder='passthrough')  # Apply no transformation to the remaining features\n",
    "\n",
    "# Split the data into training and testing sets with 80% of the data used for training and 20% for testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a machine learning pipeline\n",
    "model = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),  # First, preprocess the data\n",
    "    ('regressor', RandomForestRegressor(n_estimators=100, random_state=42))  # Then apply the RandomForest regressor\n",
    "])\n",
    "\n",
    "# Fit the model on the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model by checking its score on the training set and test set\n",
    "training_score = model.score(X_train, y_train)\n",
    "test_score = model.score(X_test, y_test)\n",
    "print(\"Training score:\", training_score)\n",
    "print(\"Test score:\", test_score)\n",
    "\n",
    "# Make predictions with the test data\n",
    "predictions = model.predict(X_test)\n",
    "print(predictions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and print the metrics for each target variable ('Likes', 'Shares', 'Comments')\n",
    "# Loop over each target variable by using enumerate to access both the index and the name of the target\n",
    "for i, target in enumerate(['Likes', 'Shares', 'Comments']):\n",
    "    # Calculate the Mean Squared Error (MSE) between the true and predicted values for each target\n",
    "    mse = mean_squared_error(y_test.iloc[:, i], predictions[:, i])\n",
    "    # Calculate the R-squared (R²) score between the true and predicted values for each target\n",
    "    r2 = r2_score(y_test.iloc[:, i], predictions[:, i])\n",
    "    # Print the results in a formatted string\n",
    "    print(f\"{target} - MSE: {mse}, R²: {r2}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
